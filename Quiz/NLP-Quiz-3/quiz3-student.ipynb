{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 3 : Instruction Tuning Model with LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minnb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from queue import PriorityQueue\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - Alpaca dataset (1 point)\n",
    "\n",
    "We will be using Alpaca dataset which contains a large corpus of text, perfect for language modeling task.\n",
    "\n",
    "[Download dataset](https://github.com/gururise/AlpacaDataCleaned/blob/main/alpaca_data.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input', 'instruction'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "import datasets\n",
    "#there are raw and preprocessed version; we used the raw one and preprocessed ourselves for fun\n",
    "dataset = datasets.load_dataset('json', data_files='./data/alpaca_data.json')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input', 'instruction'],\n",
       "        num_rows: 49401\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['output', 'input', 'instruction'],\n",
       "        num_rows: 2601\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset['train'].train_test_split(test_size=0.05, shuffle=True, seed=555)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49401"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].num_rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Preprocessing (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following prompts for fine-tuning the Alpaca model:\n",
    "\n",
    "- For examples with a non-empty input field:\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "\n",
    "- For examples with an empty input field:\n",
    "```\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Explain why saving for retirement is important.\n",
    "\n",
    "### Response:\n",
    "Saving for retirement is a vital part of your financial wellness. Retirement planning is essential if you want to maintain your current lifestyle as you transition into retirement. Retirement savings will provide you with reliable income to cover your daily expenses, medical costs, and other expenditure. \n",
    "\n",
    "Without retirement savings, you may need to rely on Social Security or other government options for income, which may not provide you with enough money to take care of all your needs. Additionally, planning for retirement allows you to take advantage of unique savings and tax benefits which can give you greater financial security. Saving for retirement is an important step to ensure you have the financial security you need when you retire.'\n",
    "```\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "The food was delicious, but the restaurant's decor was unsatisfactory.\n",
    "\n",
    "### Input:\n",
    "The food was delicious, however, the restaurant has a poor decor.\n",
    "\n",
    "### Response:\n",
    "The food was delicious, but the restaurant's decor was unsatisfactory.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tokenizing (1 point)\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['below', 'is', 'an', 'instruction', 'that', 'describes', 'a', 'task', '.', 'write', 'a', 'response', 'that', 'appropriately', 'completes', 'the', 'request', '.', '###', 'instruction', 'help', 'me', 'make', 'a', 'plan', 'for', 'when', 'i', \"'\", 'm', 'running', 'late', '###', 'response', 'when', 'you', \"'\", 're', 'running', 'late', ',', 'the', 'best', 'course', 'of', 'action', 'is', 'to', 'try', 'and', 'keep', 'your', 'cool', '.', 'take', 'a', 'deep', 'breath', ',', 'and', 'make', 'a', 'plan', 'for', 'how', 'you', 'can', 'make', 'up', 'the', 'time', '.', 'figure', 'out', 'the', 'quickest', 'route', 'to', 'your', 'destination', 'and', 'if', 'possible', ',', 'call', 'ahead', 'to', 'let', 'the', 'person', 'or', 'people', 'you', \"'\", 're', 'meeting', 'know', 'that', 'you', \"'\", 'll', 'be', 'late', '.', 'if', 'you', \"'\", 're', 'unable', 'to', 'make', 'up', 'the', 'time', ',', 'apologize', 'and', 'explain', 'why', 'you', \"'\", 're', 'running', 'late', '.', 'making', 'sure', 'to', 'be', 'as', 'honest', 'and', 'forthcoming', 'as', 'possible', '.']\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "#function to tokenize\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(f'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction: {example[\"instruction\"]}\\n### Response:{example[\"output\"]}')}\n",
    "\n",
    "#map the function to each example\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['instruction','input','output'], fn_kwargs={'tokenizer': tokenizer})\n",
    "print(tokenized_dataset['train'][333]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertion statement\n",
    "assert \"below is an instruction that describes a task\" in \" \".join(tokenized_dataset['train'][0]['tokens']), \"Word not found in tokenized dataset\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing (1 point)\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23844\n",
      "['<unk>', '<pad>', '<sos>', '<eos>', '.', 'the', 'a', ',', 'that', 'and']\n"
     ]
    }
   ],
   "source": [
    "## numericalizing\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3, specials=special_symbols)   \n",
    "\n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print(len(vocab))                         \n",
    "print(vocab.get_itos()[:10])       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Prepare the batch loader (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:         \n",
    "            #appends eos so we know it ends....so model learn how to end...                             \n",
    "            tokens = example['tokens'].append('<eos>')   \n",
    "            #numericalize          \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size #get the int number of batches...\n",
    "    data = data[:num_batches * batch_size] #make the batch evenly, and cut out any remaining                      \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data #[batch size, bunch of tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 32796]), torch.Size([128, 1719]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n",
    "# test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)\n",
    "train_data.shape, valid_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Modeling (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am using Batched Beam Search, where instead of feeding each hypothesis one by one, which takes a lot of time;  I simply concat everything into one list and feed them all at once, which is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device, pad_idx, max_length = 100):\n",
    "                \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def make_mask(self, x):\n",
    "        \n",
    "        #x = [batch size, len]\n",
    "        \n",
    "        pad_mask = (x != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #pad_mask = [batch size, 1, 1, len]\n",
    "        \n",
    "        x_len = x.shape[1]\n",
    "        \n",
    "        sub_mask = torch.tril(torch.ones((x_len, x_len), device = self.device)).bool()\n",
    "        #sub_mask = [len, len]\n",
    "            \n",
    "        mask = pad_mask & sub_mask\n",
    "        #mask = [batch size, 1, len, len]\n",
    "        \n",
    "        return mask \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, len]\n",
    "                \n",
    "        batch_size = x.shape[0]\n",
    "        x_len      = x.shape[1]\n",
    "        \n",
    "        #get mask here since we remove seq2seq class\n",
    "        mask   = self.make_mask(x)\n",
    "        #mask = [batch size, 1, len, len]\n",
    "\n",
    "        pos = torch.arange(0, x_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)          \n",
    "            \n",
    "        x = self.dropout((self.tok_embedding(x) * self.scale) + self.pos_embedding(pos))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, attention = layer(x, mask)\n",
    "        \n",
    "        #x = [batch size, len, hid dim]\n",
    "        #attention = [batch size, n heads, len, len]\n",
    "        \n",
    "        output = self.fc_out(x)\n",
    "        #output = [batch size, len, output dim]\n",
    "            \n",
    "        return output, attention\n",
    "\n",
    "    def beam_decode(self, prompt = 'Harry Potter is ', penalty_alpha = 0.9, max_length = 5, beam_size = 5):\n",
    "        \n",
    "        tokens = tokenizer(prompt)\n",
    "        indices = [SOS_IDX] + [vocab[t] for t in tokens]\n",
    "\n",
    "        decoder_input = torch.Tensor([indices]).long().to(device)\n",
    "        #decoder_input: [batch size, len] = [1, 1]\n",
    "        scores = torch.Tensor([0.]).to(device)\n",
    "        #scores: [1]\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            \n",
    "            # print(f\"========Length: {i}\")\n",
    "            \n",
    "            # Decoder prediction\n",
    "            logits, _ = self.forward(decoder_input)\n",
    "            #[beam_size, current dec len=i, vocab_size]\n",
    "                        \n",
    "            logits = logits[:, -1] \n",
    "            # Last sequence step: [beam_size, current dec len=i, vocab_size] => [beam_size, vocab_size]\n",
    "            \n",
    "            # print(f\"{logits.shape=}\")\n",
    "\n",
    "            # Softmax\n",
    "            # Log softmax is better, since beam search accumulates probability\n",
    "            # if simply softmax, the probability can get too small and then become unstable\n",
    "            log_probs = torch.log_softmax(logits, dim=1)\n",
    "    \n",
    "            # Add length penalty, otherwise, always very short sentence will win...\n",
    "            penalty   = ((5 + (i+1)) / (5 + 1)) ** penalty_alpha #see https://arxiv.org/abs/1609.08144\n",
    "            log_probs = log_probs / penalty\n",
    "            \n",
    "            # print(f\"{decoder_input[:, -1]=}\")\n",
    "            \n",
    "            # Update score where EOS has not been reached\n",
    "            log_probs[decoder_input[:, -1]==EOS_IDX, :] = -2 #discouraged it to end\n",
    "            log_probs[decoder_input[:, -1]==UNK_IDX, :] = -10 #very discouraged to spit out unk\n",
    "            scores = scores.unsqueeze(1) + log_probs \n",
    "            # scores: [beam_size, vocab_size]\n",
    "            # log_probs: [beam_size, vocab_size]\n",
    "\n",
    "            # print(f\"{log_probs.shape=}\")\n",
    "            # print(f\"{scores.shape=}\")\n",
    "            #log_probs: torch.Size([1, 29475])\n",
    "            #scores.shape=torch.Size([1, 29475])\n",
    "            \n",
    "            # Flatten scores from [beams, vocab_size] to [beams * vocab_size] to get top k, and reconstruct beam indices and token indices\n",
    "            # Since we flatten it, we have to retrieve the actual beam indices and token_indices using floor division and remainder\n",
    "            # You can try on paper; it will make sense\n",
    "            scores, indices = torch.topk(scores.reshape(-1), beam_size) #scores: [beam_size]; #indices: [beam_size]\n",
    "            beam_indices  = torch.divide   (indices, self.output_dim, rounding_mode='floor') # indices // vocab_size\n",
    "            token_indices = torch.remainder(indices, self.output_dim)                        # indices %  vocab_size\n",
    "            \n",
    "            # print(f\"{scores=}\")\n",
    "            # print(f\"{indices.shape=}\")\n",
    "            \n",
    "            # print(f\"{indices=}\")\n",
    "            # print(f\"{beam_indices=}\")\n",
    "            # print(f\"{token_indices=}\")\n",
    "            \n",
    "            # Build the next decoder input\n",
    "            # For efficiency, the trick is to concatenate all hypotheses into one string and sent to decoder at once\n",
    "            # We can later chop it ...\n",
    "            next_decoder_input = []\n",
    "            for beam_index, token_index in zip(beam_indices, token_indices):\n",
    "                # print(f\"{beam_index=}\")\n",
    "                prev_decoder_input = decoder_input[beam_index]\n",
    "                # print(f\"{prev_decoder_input=}\")\n",
    "                if prev_decoder_input[-1]==EOS_IDX:\n",
    "                    token_index = EOS_IDX # once EOS, always EOS\n",
    "                token_index = torch.LongTensor([token_index]).long().to(device)\n",
    "                next_decoder_input.append(torch.cat([prev_decoder_input, token_index]))\n",
    "                # print(\"here: \" + \" \".join([vocab.lookup_token(i) for i in next_decoder_input[-1]]) + \"; score: \" + str(scores[beam_index].item()))\n",
    "            decoder_input = torch.vstack(next_decoder_input)\n",
    "            \n",
    "            # print(f\"{decoder_input=}\")\n",
    "            \n",
    "             # If all beams are finished, and the length is at least 5, exit\n",
    "            if i > 5:\n",
    "                if (decoder_input[:, -1]==EOS_IDX).sum() == beam_size:\n",
    "                    break\n",
    "                \n",
    "        # convert the top scored sequence to a list of text tokens\n",
    "        decoder_output, _ = max(zip(decoder_input, scores), key=lambda x: x[1])\n",
    "        decoder_output = decoder_output[1:].cpu().numpy() # remove SOS\n",
    "        \n",
    "        return [vocab.lookup_token(i) for i in decoder_output if i != EOS_IDX] # remove EOS if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)        \n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        #x = [batch size, len, hid dim]\n",
    "        #mask = [batch size, 1, len, len]\n",
    "        \n",
    "        #multi attention, skip and then norm\n",
    "        _x, attention = self.self_attention(x, x, x, mask)\n",
    "        x = self.self_attn_layer_norm(x + self.dropout(_x))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        #attention = [batch size, n heads, len, len]\n",
    "    \n",
    "        #positionwise feedforward\n",
    "        _x = self.positionwise_feedforward(x)\n",
    "        x = self.ff_layer_norm(x + self.dropout(_x))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23844, 300])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple')\n",
    "fast_embedding =  fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)\n",
    "fast_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "hid_dim    = 300 # match fasttext dim \n",
    "dec_layers = 3              \n",
    "dec_heads  = 10 # hid dim % num head must be 0 \n",
    "dec_pf_dim = 512\n",
    "dec_dropout = 0.1     \n",
    "lr = 1e-3                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating model (0.5 points)\n",
    "model = Decoder(\n",
    "    vocab_size, \n",
    "    hid_dim, \n",
    "    dec_layers, \n",
    "    dec_heads, \n",
    "    dec_pf_dim, \n",
    "    dec_dropout, \n",
    "    device, \n",
    "    PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "#Applying FastText embedding to the Decoder (0.5 points)\n",
    "model.tok_embedding.weight.data = fast_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (tok_embedding): Embedding(23844, 300)\n",
       "  (pos_embedding): Embedding(100, 300)\n",
       "  (layers): ModuleList(\n",
       "    (0-2): 3 x DecoderLayer(\n",
       "      (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (fc_k): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (fc_v): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (fc_o): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "        (fc_1): Linear(in_features=300, out_features=512, bias=True)\n",
       "        (fc_2): Linear(in_features=512, out_features=300, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=300, out_features=23844, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5. Training  (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,371,480 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "        \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, _ = model(src)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    decoded_batch_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            #target = [batch size, dec len]\n",
    "\n",
    "            batch_size= src.shape[0]\n",
    "            prediction, _ = model(src)\n",
    "            #prediction = [batch size, dec len, output_dim]\n",
    "            \n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "            \n",
    "    #decoding using beam_search as example (you don't need to put here, because beam_search is for intference)\n",
    "    decoded_batch = model.beam_decode()\n",
    "    print(\"Sample beam sentence: \" + \" \".join(decoded_batch))\n",
    "            \n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is an instruction that describes a\n",
      "Epoch: 1:\n",
      "\tTrain Perplexity: 54.777\n",
      "\tValid Perplexity: 26.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is an instruction that describes a\n",
      "Epoch: 2:\n",
      "\tTrain Perplexity: 23.586\n",
      "\tValid Perplexity: 19.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is the sorcerer ' s stone\n",
      "Epoch: 3:\n",
      "\tTrain Perplexity: 18.118\n",
      "\tValid Perplexity: 17.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is an instruction that describes a\n",
      "Epoch: 4:\n",
      "\tTrain Perplexity: 15.392\n",
      "\tValid Perplexity: 16.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is an instruction that describes a\n",
      "Epoch: 5:\n",
      "\tTrain Perplexity: 13.708\n",
      "\tValid Perplexity: 15.926\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "seq_len  = 100 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}:')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load pretrained model\n",
    "model.load_state_dict(torch.load(save_path,  map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Evaluation (2 points)\n",
    "1. comparing reference (output from alpaca eval) and candidate (generated model) with ROGUE-1 (only)\n",
    "2. using 100 sample evalutions\n",
    "3. During inference, you use the user instruction with an empty input field (second option).\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765), 'rouge2': Score(precision=0.2857142857142857, recall=0.25, fmeasure=0.26666666666666666), 'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471), 'rougeLsum': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "# Import the rouge_scorer function from rouge_score\n",
    "from rouge_score import rouge_scorer\n",
    "# Initialize the scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'])\n",
    "# Compute the Rouge scores for reference and candidate.\n",
    "reference = 'The quick brown fox jumps over the lazy dog'\n",
    "candidate = 'The quick brown dog jumps on the log.' #generate by model\n",
    "scores = scorer.score(reference, candidate)\n",
    "# Print the scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minnb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for tatsu-lab/alpaca_eval contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tatsu-lab/alpaca_eval\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'generator', 'dataset'],\n",
       "    num_rows: 805\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Use this evaluation dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset(\"tatsu-lab/alpaca_eval\")['eval']\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What are the names of some famous actors that started their careers on Broadway?',\n",
       " 'output': 'Some famous actors that started their careers on Broadway include: \\n1. Hugh Jackman \\n2. Meryl Streep \\n3. Denzel Washington \\n4. Julia Roberts \\n5. Christopher Walken \\n6. Anthony Rapp \\n7. Audra McDonald \\n8. Nathan Lane \\n9. Sarah Jessica Parker \\n10. Lin-Manuel Miranda',\n",
       " 'generator': 'text_davinci_003',\n",
       " 'dataset': 'helpful_base'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.2, recall=0.023809523809523808, fmeasure=0.0425531914893617), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.2, recall=0.023809523809523808, fmeasure=0.0425531914893617), 'rougeLsum': Score(precision=0.2, recall=0.023809523809523808, fmeasure=0.0425531914893617)}\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i in range(100):\n",
    "    instruc = f'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction: {eval_dataset[i][\"instruction\"]}\\n### Response:'\n",
    "    reference = eval_dataset[i]['output']\n",
    "    candidate_full = model.beam_decode(instruc)\n",
    "    for i, word in enumerate(candidate_full):\n",
    "        if word == 'response' and candidate_full[i - 1] == '###':\n",
    "            candidate = \" \".join(candidate_full[i + 1:])\n",
    "            break\n",
    "    score = scorer.score(reference, candidate)\n",
    "    scores.append(score)\n",
    "    \n",
    "# example score\n",
    "print(scores[0])\n",
    "#It should return average ROGUE-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(list): \n",
    "    return sum(list) / len(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 scores:\n",
      "Average Precision: 0.14896409923221426\n",
      "Average Recall: 0.14896409923221426\n",
      "Average fmeasure: 0.14896409923221426\n",
      "Average overall: 0.14896409923221426\n"
     ]
    }
   ],
   "source": [
    "precisions = recall = fmeasure = []\n",
    "for sco in scores:\n",
    "    precisions.append(sco['rouge1'][0])\n",
    "    recall.append(sco['rouge1'][1])\n",
    "    fmeasure.append(sco['rouge1'][2])\n",
    "\n",
    "pre_avg = Average(precisions)\n",
    "recall_avg = Average(recall)\n",
    "f_avg = Average(fmeasure)\n",
    "overall_avg = (pre_avg + recall_avg + f_avg) / 3\n",
    "print(f\"ROUGE-1 scores:\\nAverage Precision: {pre_avg}\\nAverage Recall: {recall_avg}\\nAverage fmeasure: {f_avg}\\nAverage overall: {overall_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion (2 points (0.5 per each))\n",
    "1. State the problem why your average ROGUE-1 scores is not good?\n",
    "1. Why existing evaluation metrics (e.g, ROGUE, BLEU) don't work well with instruction tuning?\n",
    "2. How does Alpaca eval propose for evaluation?\n",
    "3. What is the problem of Alpaca eval in your opinions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The reason the average ROUGE-1 scores are not good is because of the limited model architecture complexity (due to it being from scratch) and the small number of epochs due to limited computational power. The generation also does not score well with word overlap similarity score like ROUGE-1.\n",
    "\n",
    "2. Both ROUGE and BLEU are word overlap metrics, meaning they are not good evaluation metrics for an open ended text generations tasks, such as this quiz. They work better with tasks like machine translation where the expected output is the same across all models.\n",
    "\n",
    "3. AlpacaEval calculates win-rates for models across a variety of tasks, including traditional NLP and instruction-tuning datasets, providing a comprehensive measure of model capabilities.\n",
    "AlpacaEval is a single-turn benchmark, which means it evaluates models based on their responses to single-turn prompts.\n",
    "\n",
    "4. The problem is limitations such as a bias towards longer outputs and models similar to the evaluator's base model (GPT-4 Turbo). Different models would have different generations which when compared might not be as objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
