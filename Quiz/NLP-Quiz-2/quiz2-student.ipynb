{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2 : Summarization with Seq2Seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678944384658,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "3Vo07AlhJ1Tp"
   },
   "outputs": [],
   "source": [
    "enter_name = \"Minn Banya\"\n",
    "student_id = \"st124145\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library & Dataset (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3608,
     "status": "ok",
     "timestamp": 1678944388263,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "Wt86c1YhJ1Tr",
    "outputId": "63d251d6-ea25-4745-d83b-d068475263dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.1.1\n",
    "# !pip install torchtext==0.16.1\n",
    "# !pip install datasets\n",
    "# !pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1969,
     "status": "ok",
     "timestamp": 1678944390230,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "-SgD_bU1J1Ts"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minnb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random, math, time\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "aeeb7e2754454060bfe83a5908d94600",
      "c1573faf199443d192e2d5f996054616",
      "025113dc59724811bfb88724bb8edd43",
      "b45dc527e44c45808d627a7382055aae",
      "dbc27640827b498d9de76829b0ca911c",
      "bafd0076a9344677b4d21e66153c1394",
      "035556ace0094a1db07bb17e0fd170ce",
      "2d970695aff445b29ef9402fc8b64936",
      "14578b0e69b9495ab871767777b0d790",
      "b7e910835d584f69b72443d20b662e1b",
      "4243f7f3c9f54b66b659911728d5316f"
     ]
    },
    "executionInfo": {
     "elapsed": 3983,
     "status": "ok",
     "timestamp": 1678944394210,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "EkuKq3LVJ1Tt",
    "outputId": "571b05b7-a7e9-4546-8f8e-1fc9ba4cb458"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load CNN/DM dataset from huggingface\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678944394211,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "rxEyYJcmJ1Tt"
   },
   "outputs": [],
   "source": [
    "## Load the train data and the validation data\n",
    "train_data = dataset['train']\n",
    "valid_data = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1678944394211,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "vugXbqanNU_K",
    "outputId": "8548384a-4572-4770-c70f-224db08641ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article', 'highlights']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = list(train_data.features.keys())\n",
    "columns.pop()\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678944395006,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "sSWXLQ5oNyQh",
    "outputId": "f411e5ae-9569-4d8d-ef1d-650e0e709e5b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.',\n",
       " \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking\n",
    "train_data[0]['article'], train_data[0]['highlights']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wTXuiyV_J1Tu"
   },
   "source": [
    "## Preprocessing (2 points)\n",
    "- Convert the dataset into pandas DataFrame with proper column names\n",
    "    - The training dataset is too big use only 1000 sample maximum\n",
    "    - The validation dataset is too big use only 200 sample maximum\n",
    "- lowercase everything\n",
    "- remove unnecessary words that would not make sense\n",
    "- remove linebreaks and backslashes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 12090,
     "status": "ok",
     "timestamp": 1678944407094,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "FOzGtHaQJ1Tu"
   },
   "outputs": [],
   "source": [
    "###Convert the dataset into pandas DataFrame with proper column names\n",
    "df_train =  [[i['article'], i['highlights']] for i in train_data]\n",
    "df_valid = [[i['article'], i['highlights']] for i in valid_data]\n",
    "train_df = pd.DataFrame(df_train, columns = columns)\n",
    "valid_df = pd.DataFrame(df_valid, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1678944407094,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "2UcIIa0ZJ1Tu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (200, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The training dataset is too big use only 1000 sample maximum\n",
    "### The validation dataset is too big use only 200 sample maximum\n",
    "#code here\n",
    "train_df = train_df[:1000]\n",
    "valid_df = valid_df[:200]\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1678944407095,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "-wciN_rKJ1Tv"
   },
   "outputs": [],
   "source": [
    "# Applying Lower Case to the text\n",
    "#code here\n",
    "#hint : using apply\n",
    "#traing dataset\n",
    "train_df['article']  =  train_df['article'].apply(str.lower)\n",
    "train_df['highlights'] =  train_df['highlights'].apply(str.lower)\n",
    "#validation dataset\n",
    "valid_df['article']  =  valid_df['article'].apply(str.lower)\n",
    "valid_df['highlights'] =  valid_df['highlights'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1678944407095,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "ZJG9j0WKRUEA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'has do the right thang? he has a new show'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Removing unnesscary words \n",
    "###Using Regex\n",
    "def data_cleaning(data):\n",
    "    regex_s = re.sub(\"\\\\(.+?\\\\)|[\\r\\n|\\n\\r]|-\", \"\", data)\n",
    "    fin = \" \".join(regex_s.split())\n",
    "    return fin\n",
    "\n",
    "asd = \"(cnn) has do the right thang? \\n he has -- a new show \"\n",
    "data_cleaning(asd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1678944407095,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "oJaMNrcsTQVF"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "#using above function to apply removing unnesscary words\n",
    "#hint : using apply\n",
    "train_df['article']  = train_df['article'].apply(data_cleaning)\n",
    "train_df['highlights'] =  train_df['highlights'].apply(data_cleaning)\n",
    "valid_df['article']  =  valid_df['article'].apply(data_cleaning)\n",
    "valid_df['highlights'] = valid_df['highlights'].apply(data_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 8280,
     "status": "ok",
     "timestamp": 1678944415345,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "nyjA2FIRJ1Tv"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1678944415346,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "DL93MVtLJ1Tv"
   },
   "outputs": [],
   "source": [
    "def yield_tokens(data):\n",
    "    #complete this code\n",
    "    for datasample in data:\n",
    "        yield tokenizer(datasample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1678944415346,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "FwNKd8nRJ1Tv"
   },
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3339,
     "status": "ok",
     "timestamp": 1678944418675,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "Ppnf2DquJ1Tw"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "#code here\n",
    "#assuming 'article' column has all the words in 'highlights'\n",
    "vocab_transform = build_vocab_from_iterator(yield_tokens(train_df['article']),   \n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_transform.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1678944418675,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "1Y6fdTXnJ1Tw",
    "outputId": "ccf45371-71e1-4af0-b21e-146564c0dae3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30624"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1678944418676,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "LV_iXedYJ1Tw",
    "outputId": "d9bd9fa9-66ae-44e7-8b59-1d1aac96b26d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'around'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing\n",
    "mapping = vocab_transform.get_itos()\n",
    "mapping[200]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader (1 point)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuRbou_kJ1Tw"
   },
   "source": [
    "###  FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1678944418676,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "1MwPAaWYJ1Tw"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab_transform.get_itos()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1678944418676,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "dMRRB9BpJ1Tw",
    "outputId": "23611ef6-01bc-47fc-c780-1a92c9bed749"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30624, 300])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1678944418677,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "XyVLlnpzJ1Tw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def sequencetial_transforms(*transforms):\n",
    "    #code here\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform(token_ids):\n",
    "    #code here\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "text_transform = {}\n",
    "text_transform = sequencetial_transforms(tokenizer,\n",
    "                                           vocab_transform,\n",
    "                                            tensor_transform)\n",
    "def collate_batch(batch): \n",
    "    #code here\n",
    "    src_batch,src_length, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform(src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform(trg_sample.rstrip(\"\\n\")))\n",
    "        src_length.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, torch.tensor(src_length, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1678944418677,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "QOXDjG7vJ1Tx"
   },
   "outputs": [],
   "source": [
    "class DataWrap(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataframe.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1678944418677,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "PfHuFsRVdfe9",
    "outputId": "3fd6a86a-7393-428d-e5e6-9b72d57e71af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article       london, england harry potter star daniel radcl...\n",
       "highlights    harry potter star daniel radcliffe gets £20m f...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1678944418677,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "o7MCMZFnJ1Tx"
   },
   "outputs": [],
   "source": [
    "train = DataWrap(train_df)\n",
    "valid = DataWrap(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1678944418677,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "pnJjF7y_J1Tx"
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1678944418678,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "y8wdddWQdV8D",
    "outputId": "f8bda618-1b9f-4557-bc03-acbc619a4264"
   },
   "outputs": [],
   "source": [
    "for src,_, tgt in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1008, 10]), torch.Size([62, 10]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape, tgt.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gAkAZOiWJ1Tx"
   },
   "source": [
    "## Create a model  (2 points)\n",
    "- Create a Seq2Seq attention model  \n",
    "- The Attention shoud be Multiplicative Attention  not Additive Attention\n",
    "- The main model must take \"target\" as optional. So that we dont have to pass target in inference.\n",
    "Also include max summary length, teacher forcing etc. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn       = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.fc        = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        #embedding\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #packed\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'), enforce_sorted=False)\n",
    "        #rnn\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        #unpacked\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        #-1, -2 hidden state\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim = 1)))\n",
    "        \n",
    "        #outputs: [src len, batch_size, hid dim * 2]\n",
    "        #hidden:  [batch_size, hid_dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention  = attention\n",
    "        self.embedding  = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn        = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc         = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        #input: [batch_size]\n",
    "        #hidden: [batch_size, hid_dim]\n",
    "        #encoder_ouputs: [src len, batch_size, hid_dim * 2]\n",
    "        #mask: [batch_size, src len]\n",
    "                \n",
    "        #embed our input\n",
    "        input    = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch_size, emb_dim]\n",
    "\n",
    "        #calculate the attention\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch_size, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch_size, 1, src len]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_ouputs: [batch_size, src len, hid_dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted: [batch_size, 1, hid_dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted: [1, batch_size, hid_dim * 2]\n",
    "        \n",
    "        #send the input to decoder rnn\n",
    "        #concatenate (embed, weighted encoder_outputs)\n",
    "        #[1, batch_size, emb_dim]; [1, batch_size, hid_dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input: [1, batch_size, emb_dim + hid_dim * 2]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "            \n",
    "        #send the output of the decoder rnn to fc layer to predict the word\n",
    "        #prediction = fc(concatenate (output, weighted, embed))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output   = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc(torch.cat((embedded, output, weighted), dim = 1))\n",
    "        #prediction: [batch_size, output_dim]\n",
    "            \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Attention shoud be Multiplicative Attention not Additive Attention\n",
    "\n",
    "Multiplicative Attention : \n",
    "\n",
    "$e_i = s^T W h_i \\in \\mathbb{R} ; \\textbf{W} \\in \\mathbb{R}^{d_2 × d_1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiplicative Attention\n",
    "class MultiplicativeAttention(nn.Module):\n",
    "    #code here\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, hid dim]  #this is from the decoder....so no n_layers\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len    = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        #hidden = [batch size, src len, hid dim]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "        hidden = hidden.unsqueeze(2)\n",
    "        mul = self.W(encoder_outputs).unsqueeze(3)\n",
    "        # print(hidden.shape, mul.shape)\n",
    "        energy = torch.matmul(hidden, mul)\n",
    "        #energy = [batch size, src len, hid dim]\n",
    "        # print(energy.shape)\n",
    "        attention = energy.squeeze(2)\n",
    "        attention = attention.squeeze(2)\n",
    "        #attention= [batch size, src len]\n",
    "        # print(attention.shape, mask.shape)\n",
    "        attention = attention.masked_fill(mask, -1e10)\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main model must take \"target\" as optional. So that we dont have to pass target in inference. Also include max summary length, teacher forcing etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Seq2SeqPackedAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device  = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        #src: [src len, batch_size]\n",
    "        mask = (src == self.src_pad_idx).permute(1, 0)  #permute so that it's the same shape as attention\n",
    "        #mask: [batch_size, src len] #(0, 0, 0, 0, 0, 1, 1)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, src_len, teacher_forcing_ratio=0.5, max_length = 10, trg=None):\n",
    "        #src: [src len, batch_size]\n",
    "        #trg: [trg len, batch_size]\n",
    "        \n",
    "        #initialize something\n",
    "        batch_size     = src.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        if trg is not None:\n",
    "          trg_len        = trg.shape[0]\n",
    "        else:\n",
    "           trg_len = 1\n",
    "        sum_len        = max_length \n",
    "        \n",
    "        outputs    = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device) #code here \n",
    "        attentions = torch.zeros(trg_len, batch_size, src.shape[0]).to(self.device) #code here\n",
    "\n",
    "        #send our src text into encoder\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len) \n",
    "        #encoder_outputs refer to all hidden states (last layer)\n",
    "        #hidden refer to the last hidden state (of each layer, of each direction)\n",
    "        \n",
    "        if trg is not None:\n",
    "          input_ = trg[0, :]\n",
    "        else:\n",
    "          input_ = torch.tensor(SOS_IDX)\n",
    "            \n",
    "        mask = self.create_mask(src)\n",
    "        for t in range(1, sum_len):\n",
    "            output, hidden, attention = self.decoder(input_, hidden, encoder_outputs, mask)\n",
    "            #output: [batch_size, output_dim] ==> predictions\n",
    "            #hidden: [batch_size, hid_dim]\n",
    "            #attention: [batch_size, src len]\n",
    "\n",
    "            # print(outputs.shape, output.shape)\n",
    "            #append the output to a list\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1          = output.argmax(1)  #autoregressive\n",
    "            \n",
    "            if trg is not None: \n",
    "              input_ = trg[t] if teacher_force else top1\n",
    "            else:\n",
    "              input_ = top1 #code here\n",
    "\n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678944418679,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "mbLE5jfOJ1Tx"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_xpjYUW6J1Ty"
   },
   "source": [
    "### Quick Note: Apply the fasttext embedding to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2175,
     "status": "ok",
     "timestamp": 1678944420845,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "-9R4rZhQJ1Ty",
    "outputId": "146d89a9-0612-440c-c61f-4a3ce4a37f4a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqPackedAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(30624, 300)\n",
       "    (rnn): GRU(300, 300, bidirectional=True)\n",
       "    (fc): Linear(in_features=600, out_features=300, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): MultiplicativeAttention(\n",
       "      (W): Linear(in_features=600, out_features=300, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(30624, 300)\n",
       "    (rnn): GRU(900, 300)\n",
       "    (fc): Linear(in_features=1200, out_features=30624, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(vocab_transform)\n",
    "output_dim = len(vocab_transform)\n",
    "emb_dim =  300 #code here\n",
    "hid_dim =  512 #code here\n",
    "dropout = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = MultiplicativeAttention(hid_dim)\n",
    "enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn)\n",
    "\n",
    "#Applying FastText embedding to the Encoder and the Decoder\n",
    "#code here\n",
    "enc.embedding.weight.data = fast_embedding\n",
    "dec.embedding.weight.data = fast_embedding\n",
    "\n",
    "model = Seq2SeqPackedAttention(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1678944420846,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "lHENzAP1J1Ty",
    "outputId": "0e3679ae-ea0f-4601-ca98-1f2b4d9c6908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9187200\n",
      "270000\n",
      "270000\n",
      "   900\n",
      "   900\n",
      "270000\n",
      "270000\n",
      "   900\n",
      "   900\n",
      "180000\n",
      "   300\n",
      "180000\n",
      "   300\n",
      "9187200\n",
      "810000\n",
      "270000\n",
      "   900\n",
      "   900\n",
      "36748800\n",
      " 30624\n",
      "______\n",
      "57679824\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678944420846,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "X1vODmbwJ1Ty"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Ploting (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678944420846,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "NC94QbkVJ1Ty"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    #code here\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src,src_length, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, attentions = model(src, src_length, trg = trg)\n",
    "        \n",
    "        #trg    = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        # print(output.shape, trg.shape)\n",
    "        #the loss function only works on 2d inputs with 1d targets thus we need to flatten each of them\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg    = trg[1:].view(-1)\n",
    "        #trg    = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        # print(output.shape, trg.shape)\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678944420847,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "6Qdbtk5-J1Ty"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "        \n",
    "    #turn off dropout (and batch norm if used)\n",
    "    #code here\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_length, trg in loader:\n",
    "            \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, attentions = model(src, src_length, trg = trg, teacher_forcing_ratio = 0) #turn off teacher forcing\n",
    "\n",
    "            #trg    = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg    = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "executionInfo": {
     "elapsed": 3452,
     "status": "ok",
     "timestamp": 1678944424294,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "yXtBAVijJ1Ty"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_length = len(list(iter(train_loader))) \n",
    "valid_loader_length = len(list(iter(valid_loader))) \n",
    "train_loader_length, valid_loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678944424295,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "pRc1II-LJ1Ty"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AsHnh_zJJ1Tz",
    "outputId": "2ee89313-025d-4807-b21e-a37681aed310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 32s\n",
      "\tTrain Loss: nan | Train PPL:     nan\n",
      "\t Val. Loss: nan |  Val. PPL:     nan\n"
     ]
    }
   ],
   "source": [
    "#code here\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 1\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, valid_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZORXcgSRJ1Tz"
   },
   "source": [
    "### Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "arYG5DCoJ1Tz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code here\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GcKzdjSdJ1Tz"
   },
   "source": [
    "## Finally we can summarize (1 points)\n",
    "- Summarize the given sample (The max length of summary should be 20 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "-zYEaQKzJ1Tz"
   },
   "outputs": [],
   "source": [
    "###Summarize the given sample\n",
    "max_len = 20\n",
    "sample = \"\"\"Mrs Kopta, who has dementia, has been living in a nursing home after she was taken in as a person in need seven years after she disappeared. \n",
    "It's a relief she hasn't been murdered. Her husband Bob Kopta said he had been married to Mrs Kopta for 20 years before she went missing. \n",
    "He said: It's a relief knowing that she's not laying in a ditch somewhere, or murdered somewhere. \n",
    "The 86-year-old added that his family suspected she may be in Puerto Rico but she was declared dead around 25 years ago. \n",
    "The retired electrician also said he had consulted with a psychic about her whereabouts. \n",
    "Mrs Kopta has two sisters - a twin, who died six years ago, and a younger sister who was relieved to learn she's still alive, Mr Kopta added. \n",
    "He said he has experienced a range of emotions over the years but is content knowing his wife is alive and being cared for. \n",
    "After 30 years, you try to forget about it. Now I can forget about it. \n",
    "We know what happened, and she is taken care of now, he said.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower case\n",
    "#clean text as well\n",
    "processed_sample = data_cleaning(sample.lower())\n",
    "input = text_transform(processed_sample).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m output, attention \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[119], line 33\u001b[0m, in \u001b[0;36mSeq2SeqPackedAttention.forward\u001b[1;34m(self, src, src_len, teacher_forcing_ratio, max_length, trg)\u001b[0m\n\u001b[0;32m     30\u001b[0m attentions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(trg_len, batch_size, src\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m#code here\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#send our src text into encoder\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m encoder_outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_len\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#encoder_outputs refer to all hidden states (last layer)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#hidden refer to the last hidden state (of each layer, of each direction)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[70], line 15\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, src, src_len)\u001b[0m\n\u001b[0;32m     13\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(src))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#packed\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m packed_embedded \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpack_padded_sequence(embedded, \u001b[43msrc_len\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#rnn\u001b[39;00m\n\u001b[0;32m     17\u001b[0m packed_outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(packed_embedded)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "#code here\n",
    "model.eval()\n",
    "input = input.to(device)\n",
    "output, attention = model(input,input.size(0))\n",
    "output = output.squeeze(1)\n",
    "output = output[1:]\n",
    "output_max = output.argmax(1)\n",
    "mapping = vocab_transform.get_itos()\n",
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pqhRJ-eCJ1Tz"
   },
   "source": [
    "## Conclusion (2 points)\n",
    "- How did the model perform?\n",
    "- Does using the FastText Embedding improve the performance?\n",
    "- What do you suggest we can do to increase the performance in text summarization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performed quite bad due to limited resource and training.\n",
    "FastText embedding improve the performance signitficantly\n",
    "We can increase training size, epoch and learning rate decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "025113dc59724811bfb88724bb8edd43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d970695aff445b29ef9402fc8b64936",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14578b0e69b9495ab871767777b0d790",
      "value": 3
     }
    },
    "035556ace0094a1db07bb17e0fd170ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14578b0e69b9495ab871767777b0d790": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d970695aff445b29ef9402fc8b64936": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4243f7f3c9f54b66b659911728d5316f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aeeb7e2754454060bfe83a5908d94600": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c1573faf199443d192e2d5f996054616",
       "IPY_MODEL_025113dc59724811bfb88724bb8edd43",
       "IPY_MODEL_b45dc527e44c45808d627a7382055aae"
      ],
      "layout": "IPY_MODEL_dbc27640827b498d9de76829b0ca911c"
     }
    },
    "b45dc527e44c45808d627a7382055aae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7e910835d584f69b72443d20b662e1b",
      "placeholder": "​",
      "style": "IPY_MODEL_4243f7f3c9f54b66b659911728d5316f",
      "value": " 3/3 [00:00&lt;00:00, 95.45it/s]"
     }
    },
    "b7e910835d584f69b72443d20b662e1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bafd0076a9344677b4d21e66153c1394": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1573faf199443d192e2d5f996054616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bafd0076a9344677b4d21e66153c1394",
      "placeholder": "​",
      "style": "IPY_MODEL_035556ace0094a1db07bb17e0fd170ce",
      "value": "100%"
     }
    },
    "dbc27640827b498d9de76829b0ca911c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
